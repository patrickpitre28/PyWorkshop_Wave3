{
    "cells": [
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Import packages and verify versions"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import pandas as pd\nimport sklearn\nprint('The pandas version is {}.'.format(pd.__version__))\nprint('The scikit-learn version is {}.'.format(sklearn.__version__))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Load data"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Import necessary IBM CoS libraries and insert customers.csv into the data frame.\n\nThis can be accomplished within Watson Studio on the right hand menu, find the applicable file and chose Pandas Data Frame\n\n*Default is to insert into a data frame named df_data_1. When using a templated notebook, make sure that the cells below reference the correct data frame name OR change this cell to name the data frame in the same name used below*"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Insert the transactions.csv into a data frame\n\nThe same note regarding the default data frame name applies here. The libraries do not need to be re-imported (though the insert to code from the right hand menu may still add the import statement.)"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Print the number of rows for the customer data frame & transaction data frame\nThis is to confirm that the two data frames will join (same # of rows)\n(hint: use the same key column to ensure the same number of rows)"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Join Data"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Join the Customer and Transaction Data Frames together. Identify which key should be used and the type of join (inner, outer) to be used. *Do not forget to drop the index*"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Display the number of rows joined and examine the newly joined data set. \n*Number of rows joined should be 788*"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Analyze Data\n\nLook for column outliers or other quality issues, such as data type, that will affect the end model\n\n*Should be clear to see that one of the columns contains data outliers*"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Check your data frame's data types\n\nAre the types correct? You can either check the entire data frame, or specific columns that may have been identified from the df.head() command ran earlier following the join"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Data Cleansing\n\n### Let's start with dropping rows that have invalid data\n\nWe do this so that we don't use resources cleansing rows that we will ultimately discard due to invalid data\n\n*We may also want to save the rows with invalid data for correction and then re adding them. For the purpose of this lab, the data will just be dropped*"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Show rows with invalid data"
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Drop the rows with invalid data\n\nDisplay the remaining number of rows in the data set. *Should be 761*"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Now we want to correct the data type of the column identified above.\n\nConvert String Data to Numeric data using the .to_numeric function of pandas."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Force the Frequency_score column to a numeric data type as it should be\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Check the new data type of the column we changed \n\nWe want to make sure that the new data type is inline with similar columns"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#Frequency_score is float data type, but should be integer\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Cast the column to the correct data type \n*hint - should be the same dat type as a similarly named column*"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Cast Frequency_score as integer\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Now we want to remove the outliers from the column identified above in the .decribe() command\nwe can see that 75% of the rows fall within the 1-4 number range. We want to capture as much of the data as applicable, so let's drop any rows that are less than 5"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Remove the rows with outliers in Monetary_score that we previously identified\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Let's spot check the new data frame and the specific column we removed the outliers from"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Now lets drop columns not being used as features\n*The columns not being used are CustomerID, Invest, Educ, MARTIAL, TimeYears, Lasttrans, current, Monetary_score*\n\nOnce dropped, we will want to check the new data frame"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Convert numeric data to integer (some numeric columns were inferred as float64)\nFirst, lets check the data type of the column(s)."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Retire column was infered as a float data type\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Let's create a list of column names based on numeric data types (*hint float64 & int64). We also want to create a list of the correct data type to correspond with the list of column names"
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Create a dictionary that will be used to set the numeric columns to integer type & Print the dictionary"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Convert the numeric columns to integer"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Check the data types to insure all numeric data is int64"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Encode the string data"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#install sklearn-pandas package that will be used to encode the categorical features\n!pip install sklearn-pandas\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn_pandas import DataFrameMapper",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Encode the Churn label calling the new column CHURN and drop the original Churn column"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Apply the LabelEncoder to encode the categorical features"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Prepare the data for machine learning\n\n1. Split the label column out from the features dataframe\n2. Sample the indexed DataFrame\n3. Create a separate DataFrame from the label column and sample"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Split the label column out from the features dataframe\n\n\n# Sample the indexed DataFrame\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Create training and test datasets\u00b6\n\nSplit X and y Data Frames into training and testing sets - accomplished by importing test_train_split from the sklearn.model_selection library (code already included)"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# split X and y into training and testing sets\nfrom sklearn.model_selection import train_test_split\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Show the number of rows in each data set"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Train the model\n\nThe eval_metric parameter specifies the evaluation metrics for validation data\nWe will be using a Binary classification error rate. It is calculated as # of wrong cases divided by # of all cases. *(#wrong/#all)*\nFor the predicitons, the evaluation will regard the instances with prediction value larger than 0.5 as positive instances and\nthe others as negative instances. "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import sklearn.pipeline\nfrom xgboost import XGBClassifier\n\n#Set the classifer, the stpes and the pipeline\n\n\n#Train the model\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Show model training parameters "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Check model accuracy\n\n1. import the required modules from the scikit-learn metrics package *(provided)*\n2. make predictions for the test data\n3. convert the numpy array provided the from the precit function to a list\n4. evaluate the predictions"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#import required modules from the scikit-learn metrics package\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# make predictions for test data\n\n# Convert numpy array to list\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# evaluate predictions\n\n# Show the accuracy\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Analyze Model - Feature Importance and Trees"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Show the feature importances - will result in a list of decimals\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Print the importance based on columns\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#Import libraries for plot importance visualization\nfrom xgboost import plot_importance\nplot_importance(XGBClassifier)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Install graphviz for visual analysis & import matplotlib.pylot *(provided)* "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "!pip install graphviz\nimport matplotlib.pyplot as plt\n%matplotlib inline",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Investigate model"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Plot and display the performance evaluation\n\n\n# Set the figure for matplotlib figures (provided)\nfig, ax = plt.subplots(1, 1, sharex=True, figsize=(8, 6))\n\nax.plot(eval_steps, [1-x for x in eval['validation_0']['error']], label='Train')\nax.plot(eval_steps, [1-x for x in eval['validation_1']['error']], label='Test')\nax.legend()\nax.set_title('Accuracy')\nax.set_xlabel('Number of iterations')",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Avoid Overfitting By Limiting Number of Trees"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# ntree_limits the number of trees in the prediction; defaults to 0 (use all trees), we want to increase the limit to 10\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Check the accuracy of the trained model\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Avoid Overfitting By Early Stopping"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Validation error needs to decrease at least every <early_stopping_rounds> round(s) to continue training\n# Returns the model from the last iteration (not the best one) \n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Show best score\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Show best number of trees\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Check the accuracy of the trained model with early stopping\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Plot Model Performance"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Print the confusion matrix\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Plot the confusion matrix\nplt.matshow(cm)\nplt.title('Confusion matrix')\nplt.colorbar()\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Plot the ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('ROC curve (Logistic Regression)')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.grid(True)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Print out AUC, the percentage of the ROC plot that is underneath the curve\n",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}