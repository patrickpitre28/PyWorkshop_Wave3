{
    "cells": [
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Analyze industrial sensor data\n\nThis notebook provides some guidance for the Industrial Data Analysis lab exercise in the PyR training, this to help you finish the exercise in the designated time. If you have advanced Python and Pandas skills, we recommend to start with an empty notebook and work through the exercise self-sufficiently."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Steps\nThe notebook passes through the following steps in reaching the ultimate end-goal: building an Python model that can be used to detect anomalies in one of the sensors: `Wheel Front Temp Celsius`"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Import required packages\nEven though you can import new packages anywhwere in the notebook, it is common practice move the imports to the beginning of the notebook. This to immediately show the dependencies when someone else opens it.\n\nAdditionally, when you finished working on the notebook:\n* Restart the kernel (Kernel --> Restart)\n* Re-run the notebook (Run --> Run All Cells)\n\nThis is to ensure the notebook will also run if cells are not executed in the exact same sequence when you developed it. Very often, the exploration and analysis of data is an iterative process and you may end up renaming variables, updating dataframe columns. By running the full notebook from start to end and validating the end result, you will avoid errors when you need to use it later."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 1,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Read input files\nIn this example, the files are directly read from the GitHub repository into a Pandas dataframe. Even though you could upload the file to the notebook server (Watson Studio or other), you will find it as easy to load the data directory from the internet.\n\nThe following 3 files have been made available:\n* readings: http://raw.githubusercontent.com/fketelaars/pyr-industrial/master/readings.csv\n* sensors: http://raw.githubusercontent.com/fketelaars/pyr-industrial/master/sensor.csv\n* devices: http://raw.githubusercontent.com/fketelaars/pyr-industrial/master/device.csv"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CDDE TO LOAD DATA INTO PANDAS DATAFRAMES HERE #",
            "execution_count": 2,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Now that the data has been loaded into the 3 Pandas dataframes, check that the number of records match the expectations.\n* readings: ~326k entries\n* sensors: 27 entries\n* devices: 4 entries"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 3,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## View the first few rows of every data set\nShow the top 5 rows of every dataframe"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 4,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 5,
            "outputs": []
        },
        {
            "metadata": {
                "scrolled": false
            },
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 6,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Convert epoch timestamp to datetime\nExtend the \"readings\" dataframe with a timestamp column which represents the human readable date and time for the `tsepoch` column. If you haven't noticed yet, the `tsepoch` timestamp is expressed in milliseconds."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 7,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Determine time span of the readings\nNow that you've loaded the data, find the following properties for the readings:\n* The number of seconds that the readings span\n* The lowest timestamp in human readable format\n* The highest timestamp in human readable format"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 8,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Determine number of readings for each sensor\nNot every sensor has the same sample rate. Later you will have to resample the readings to ensure you have a value for every time interval. Find the number of readings for every sensor and display this in a chart.\n\n**Tip**: When using `matplotlib`, don't forget to add an instruction to tell the notebook that charts should be shown in-line.\n"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 9,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 10,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 11,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Determine statistics for each sensor and write to new \"sensors\" CSV file\nThe `sensor.csv` file contained placeholders for the count, minimum, maximum and mean of every sensor. Not all of these values were populated. You will need the sensor statistics later but you do not necessarily need to store them. If you run the notebook in the cloud, you can consider to try and persist the data in the object store, but retrieving the values in a dataframe is sufficient."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 12,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Once you have retrieved the count, minimum, maximum and mean, join this data with the original sensors dataframe so that you can persist it as a csv file."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 13,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "The resulting dataframe should have the following columns:\n* sensor_id\n* description\n* low_value\n* high_value\n* mean_value\n\n**Tip**: To be able to join, you may have to reset the index of the dataframe holding the aggregated values, or you may have to use the index of that dataframe to join with the sensors dataframe."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 14,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Visualization"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Determine which sensors make sense to visualize\nSensors with a constant value can be ignored and should be dropped before visualizing. Use the sensor statistics you retrieved above to determine if sensors have a constant value. Drop the readings of those sensors.\n\n**Tip**: When dropping rows or columns, you may get a warning that the original dataframe could be affected. Use the `copy()` function to make a copy of the original dataframe before deleting rows or columns."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 15,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "If your code is correct, approximately 321900 readings should remain in the new dataframe."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Plot some of the sensors\nPick a couple of sensor IDs (for example: 14, 27 and 68) and plot the values. Use the human readable timestamp for the x-axis."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 16,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Down-sample the different readings\nBefore we can start looking at correlations between different sensors, we need to match the timestamps of the readings from the different sensos. Let's try and down-sample the variable readings to create a pivoted dataframe with a value for every sensor and every timestamp.\n\nTo lose as little detail as possible, we will down-sample to 0.2 seconds. This means there will be 864 * 5 readings for every sensor.\n\n**Tip**: Use the pandas `resample` function and `groupby` to down-sample the readings to 200 milliseconds."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 17,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "You may find that some of the sensors did not have a reading for every 200 ms interval, which means that a `NaN` value is generated. Remove these readings from the resulting dataframe."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 18,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Check that you more or less have the same number of samples for each sensor now."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Find correlations between sensors\nNow that we have the equivalent number of readings for every sensor, you can find correlcations between the different sensor IDs. You will need to match up the readings for different sensors with each other.\n\n**Tip**: Use the pandas `pivot_table` function to get 1 column for every sensor. Every row will have a timestamp and value for each of the sensors."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 19,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "The column names now reference the `sensor_id` which really is not that meaningful. Before correlating, let's give the columns some meaningful names.\n\n**Tip**: Join the numeric column names with the `sensors` data to retrieve the names."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 20,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Once you have changed the column names, show the first 5 rows of the resulting dataframe."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 21,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Build the correlations table"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 22,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Show correlations in heatmap\nThe above correlation diagram is a little difficult to read. It is better to convert this into a heatmap.\n\n**Tip**: The Seaborn package has some nice heatmaps that are easy to use. https://seaborn.pydata.org/"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 23,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "From the heat map you should see that `Wheel Front Speed RPM` and `Wheel Front Temp Celsius` are strongly correlated, let's plot this in a regression plot."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 24,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "The covariance between the speed and temperature should be clearly visible in the chart."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Train model on the data\n\nNow that we have found a correlation, let's try to build a model we can use for predictions."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Import additional packages\nYou can choose to import additional packages here, but in the end it is recommended to move all imports to the top of the notebook."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 25,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Split the data in a training and test set\nIt is best to start from the pivoted dataframe. If you didn't do so before, you first have remove any NaN values from the overall pivoted dataframe, otherwise the training or testing of the model will fail."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 26,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Now split the dataframe into a training and test set. Please note that the independent variables (features) and dependent variables (labels) must end up in different dataframes/series.\n\n**Tip**: SciKit Learn has a nice function that will split up a dataframe in training and testing data, and also separate features from labels."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 27,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### First try simple linear model\n\nTry to fit a simple linear regression model against the data. When the model has been fit, calculate the R<sup>2</sup> score.\n\n**Tip**: When you try this for the first time, SciKit Learn may throw errors because the shape of the training and test data is not what it expects. With Pandas you can re-shape the data to the desired format using the `reshape()` function."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Fit the linear model."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 28,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Calculate the R<sup>2</sup> score."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 29,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Try to improve the score with a polynomial regression\nTo use a polynomial regression, it is best to create a pipeline that will first generate the polynomial features and then train."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "You can choose to import additional packages here, but in the end it is recommended to move all imports to the top of the notebook."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 30,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Define the pipeline and fit the model. When the model has been trained, calculate the R<sup>2</sup> score and see if it has improved compared to the simple linear model. Based on the degree of polynomial features the score will improve or not. With the provided data set, a degree of 3 was optimal."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 31,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Now plot everything in 1 diagram\nYou should now have 2 models. It would be good to visualize how well the models can predict the value of temperature given the speed."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "First assemble a dataframe that will have the following columns:\n* speed\n* actual temperature\n* predicted temperature for the simple linear regression\n* predicted temparature for the polynomial regression"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 32,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Once you have the dataframe with these columns, plot the values. Try to improve the visualization by:\n* Choosing a figure size that will fit the width of your screen\n* Choose different colours for the plotted points\n* Create a legend that explains the values and colours shown in the chart"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# YOUR CODE HERE #",
            "execution_count": 33,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Optional exercise or demo\nScore live data and show in a chart."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.8",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}